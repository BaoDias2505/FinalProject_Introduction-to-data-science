{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "\n",
    "# Google API\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"AIzaSyCJ39THFW-3CAzy4cbo4d8kts1ZuOeoyOM\" #vmphat24\n",
    "# api_key = \"AIzaSyACknQ39Hmkq_M41-MAXM-iIb2phykDEvY\" #pvminh\n",
    "# api_key = \"AIzaSyD1plVeU_boVWAezOXzkLal91NfaiPIf3M\" #vmphat21clc\n",
    "# api_key = \"AIzaSyA9N1-DIVh6DJQf430_bOvwRaJte0QOjl8\" #ngocquynhhh\n",
    "# api_key = \"AIzaSyD7nKX_j2-7SjlOANbWcocik1M5BPxyv5I\" #tdtkiet.ytapi\n",
    "# api_key = \"AIzaSyCHpj5kbqp_fb7QmdF44ZwgxBFn7u-Jw1w\" #frannievo.ytapi\n",
    "# api_key = \"AIzaSyDz6Zx6KmNbBnolwfbqYJATx98XKTPA62E\" #danieltran.ytapi\n",
    "# api_key = \"AIzaSyD_KCeU6mNUxpI6hOa8gsUNGekKPnb0Fv8\" #tulindao.ytapi\n",
    "# api_key = \"AIzaSyAGaUKApQRIBfn4Hf4xkOnra8y1SIR0tUY\" #martincung.ytapi\n",
    "api_key = \"AIzaSyB94reWaXOBvZHD76KgQeUj5P1eibtaqOM\" #giabao\n",
    "\n",
    "\n",
    "# Tool to get youtube channel id: https://www.streamweasels.com/tools/youtube-channel-id-and-user-id-convertor/\n",
    "channel_ids = [\n",
    "    # # ==================== pvminh ====================\n",
    "    # 'UCtYLUTtgS3k1Fg4y5tAhLbw', # StatQuest with Josh Starmer\n",
    "    # 'UCYO_jab_esuFRV4b17AJtAw', # 3Blue1Brown\n",
    "    # 'UCCezIgC97PvUuR4_gbFUs5g', # Corey Schafer\n",
    "    # 'UCfzlCWGWYyIQ0aLC5w48gBQ', # Sentdex\n",
    "    # 'UCh9nVJoWXmFb7sLApWGcLPQ', # codebasics \n",
    "    # 'UCNU_lfiiWBdtULKOw6X0Dig', # Krish Naik\n",
    "    # 'UCzL_0nIe8B4-7ShhVPfJkgw', # DatascienceDoJo\n",
    "    # 'UCLLw7jmFsvfIVaUFsLs8mlQ', # Luke Barousse \n",
    "    # 'UCiT9RITQ9PW6BhXK0y2jaeg', # Ken Jee\n",
    "    # 'UC7cs8q-gJRlGwj4A8OmCmXg', # Alex the Analyst\n",
    "    # 'UC2UXDak6o7rBm23k3Vv5dww', # Tina Huang\n",
    "    # 'UCxX9wt5FWQUAAz4UrysqK9A', # CS Dojo\n",
    "    # 'UCEBpSZhI1X8WaP-kY_2LLcg', # 365 Data Science\n",
    "    # 'UCV8e2g4IWQqK71bbzGDEI4Q', # Data Professor\n",
    "    # 'UCnVzApLJE2ljPZSeQylSEyg', # Data School\n",
    "    # 'UCBPRJjIWfyNG4X-CRbnv78A', # Abhishek Thakur\n",
    "    # 'UCUcpVoi5KkJmnE3bvEhHR0Q', # ritvikmath\n",
    "    # 'UCJQJAI7IjbLcpsjWdSzYz0Q', # Thu Vu data analytics\n",
    "    # 'UC6AVa0vSrCpuskzGDDKz_EQ', # zedstatistics\n",
    "    # 'UCnz-ZXXER4jOvuED5trXfEA', # techTFQ\n",
    "    # # -------------------- 1-20 --------------------\n",
    "\n",
    "    # # ==================== vmphat21clc ====================\n",
    "    # 'UCHXa4OpASJEwrHrLeIzw7Yg', # Nicholas Renotte\n",
    "    # 'UC5_6ZD6s8klmMu9TXEB_1IA', # CodeEmporium \n",
    "    # 'UCvZnwzmc3m1Eush-Or8Z6DA', # Shashank Kalanithi\n",
    "    # 'UCkYooZtwK_RJAd9SdL1jfeA', # The High ROI Data Scientist\n",
    "    # 'UCFrjdcImgcQVyFbK04MBEhA', # Brandon Foltz\n",
    "    # 'UCY8mzqqGwl5_bTpBY9qLMAA', # Andreas Kretz\n",
    "    # 'UC68KSmHePPePCjW4v57VPQg', # Python Programmer\n",
    "    # 'UCJublDh2UsiIKsAE1553miw', # Greg Hogg\n",
    "    # 'UCVhQ2NnY5Rskt6UjCUkJ_DA', # ArjanCodes\n",
    "    # 'UCh8IuVJvRdporrHi-I9H7Vw', # Unfold Data Science\n",
    "    # 'UCcQx1UnmorvmSEZef4X7-6g', # Jay Feng\n",
    "    # 'UCH6gDteHtH4hg3o2343iObA', # Analytics Vidhya\n",
    "    # 'UCR1-GEpyOPzT2AO4D_eifdw', # Jeff Heaton\n",
    "    # 'UCteRPiisgIoHtMgqHegpWAQ', # Sundas Khalid \n",
    "    # 'UC58v9cLitc8VaCjrcKyAbrw', # Machine Learning with Phil\n",
    "    # 'UCW8Ews7tdKKkBT6GdtQaXvQ', # StrataScratch\n",
    "    # 'UCMLtBahI5DMrt0NPvDSoIRQ', # Machine Learning Street Talk\n",
    "    # 'UCb0qAKEAwNC0FNatapc-yZg', # YUNIKARN\n",
    "    # 'UCeiiqmVK07qhY-wvg3IZiZQ', # David Robinson\n",
    "    # 'UCRqCK8izkO5xeVVtMKSHeRQ', # Data Nash\n",
    "    # # -------------------- 21-40 --------------------\n",
    "\n",
    "    # # ==================== ngocquynhhh ====================\n",
    "    # 'UCObs0kLIrDjX2LLSybqNaEA', # Great Learning\n",
    "    # 'UC79Gv3mYp6zKiSwYemEik9A', # DataCamp - Data Camp\n",
    "    # 'UCFp1vaKzpfvoGai0vE5VJ0w', # Guy in a Cube\n",
    "    # 'UC4JX40jDee_tINbkjycV4Sg', # Tech With Tim\n",
    "    # 'UCs10x-muRrTQMJ4Ya-fmIlw', # Snowflake Inc.\n",
    "    # 'UCGoxKRfTs0jQP52cfHCyyRQ', # MITCBMM\n",
    "    # 'UCJINtWke3-FMz2WuEltWDVQ', # Applied AI Course\n",
    "    # 'UCUzGQrN-lyyc0BWTYoJM_Sg', # What's AI by Louis Bouchard\n",
    "    # 'UCsBKTrp45lTfHa_p49I2AEQ', # Brandon Rohrer\n",
    "    # 'UCxladMszXan-jfgzyeIMyvw', # Rob Mulla\n",
    "    # 'UCCR6F6X28Kj00bgPCH9Ct_w', # Data Science with Sharan\n",
    "    # 'UCV0qA-eDDICsRR9rPcnG7tw', # Joma Tech\n",
    "    # 'UCtslD4DGH6PKyG_1gFAX7sg', # Alexander Amini\n",
    "    # 'UCn8ujwUInbJkBhffxqAPBVQ', # Dave Ebbelaar\n",
    "    # 'UC-YAxUbpa1hvRyfJBKFNcJA', # Leo Isikdogan\n",
    "    # 'UCwB7HrnRlOfasrbCJoiZ9Lg', # The Semicolon\n",
    "    # 'UCgBncpylJ1kiVaPyP-PZauQ', # Serrano.Academy\n",
    "    # 'UCVqU1Vy3HO4Ms-pbN0r2_kg', # Recall by Dataiku\n",
    "    # 'UCenqe6Cvfd47aHAOb9Qe8yA', # Damsel in Data\n",
    "    # 'UCNIkB2IeJ-6AmZv7bQ1oBYg', # Arxiv Insights\n",
    "    # # -------------------- 41-60 --------------------\n",
    "    \n",
    "    # # ==================== tdtkiet.ytapi ====================\n",
    "    # 'UCtY8JjMQpzYb5FFvUr2JnUw', # The Data Incubator\n",
    "    # 'UCX7Y2qWriXpqocG97SFW2OQ', # Jeremy Howard\n",
    "    # 'UC3q8O3Bh2Le8Rj1-Q-_UUbA', # Databricks\n",
    "    # 'UCn1USB9-5UqKJTSHd1JGcVw', # BEPEC by Kanth\n",
    "    # 'UChMU-aFKCoQyPOQzYph35YA', # The Engineer Guy 2.0\n",
    "    # 'UCmLGJ3VYBcfRaWbP6JLJcpA', # Seattle Data Guy\n",
    "    # 'UC9Wi1Ias8t4u1OosYnHhi0Q', # Hsuan-Tien Lin\n",
    "    # 'UCkzW5JSFwvKRjXABI-UTAkQ', # Aladdin Persson\n",
    "    # 'UC4UJ26WkceqONNF5S26OiVw', # deeplizard \n",
    "    # 'UCYoS2VT03weLA7uzvL2Vybw', # Alex Smola\n",
    "    # 'UCqd6TofKNjqagInm5Waeu7w', # Springboard\n",
    "    # 'UCqBbIn5Er4HevFg1wkeu-8A', # AI Planet\n",
    "    # 'UC-HLXw5cFC-7zqaXqTIlj-g', # Satyajit Pattnaik\n",
    "    # 'UC0g9jkx4MwsojJfBt1MnWew', # MYANMAR DATA SCIENCE\n",
    "    # 'UCZHmQk67mSJgfCCTn7xBfew', # Yannic Kilcher\n",
    "    # 'UCRjtBP-o5FbgRzX2BHQEFtQ', # Chai Time Data Science\n",
    "    # 'UCr8O8l5cCX85Oem1d18EezQ', # Daniel Bourke \n",
    "    # 'UC8ofcOdHNINiPrBA9D59Vaw', # Bhavesh Bhatt\n",
    "    # 'UC34rW-HtPJulxr5wp2Xa04w', # DigitalSreeni\n",
    "    # 'UCb1GdqUqArXMQ3RS86lqqOw', # iNeuron Intelligence\n",
    "    # # -------------------- 61-80 --------------------\n",
    "\n",
    "    # # ==================== frannievo.ytapi ====================\n",
    "    # 'UC3rY5HOgbBvGmq7RnDfwF7A', # Rishabh Mishra\n",
    "    # 'UCk5tiFqPvdjsl7yT4mmokmg', # Data Science Tutorials\n",
    "    # 'UCHGw1uT1XmqaRm-6W15-KlQ', # Data Science Basics\n",
    "    # 'UCAEgip72UcvYwjcqzcJ1I2g', # DataScience RoadMap\n",
    "    # 'UCkp0ctv0vCNfh7i7D9GnHhw', # The Data Science Channel\n",
    "    # 'UCyU1CDYl_NX8nsxCCkKfk3A', # FUN WITH DATA SCIENCE\n",
    "    # 'UCq6XkhO5SZ66N04IcPbqNcw', # Keith Galli \n",
    "    # 'UCR1bgsuXHmXWDXJlua1mK4Q', # Kanika Jindal\n",
    "    # 'UCw_LFe2pS8x3NyipGNJgeEA', # Learn with Lukas\n",
    "    # 'UCDybamfye5An6p-j1t2YMsg', # Data With Mo\n",
    "    # 'UCTRB_OkdfGEA80HyLYLL2UQ', # DataTrained\n",
    "    # 'UCBtOvx6gen_SlIjKtWdQZmw', # Data Science\n",
    "    # 'UC0GmdVKZhMM3Rmielp4oVAA', # Stefanovic \n",
    "    # 'UCcIXc5mJsHVYTZR1maL5l9w', # DeepLearningAI\n",
    "    # 'UCG04dVOTmbRYPY1wvshBVDQ', # Siddhardhan\n",
    "    # 'UCwgKmJM4ZJQRJ-U5NjvR2dg', # george hotz archive\n",
    "    # 'UCtatfZMf-8EkIwASXM4ts0A', # AssemblyAI\n",
    "    # 'UCCWi3hpnq_Pe03nGxuS7isg', # CampusX\n",
    "    # 'UCHNO_Y3DskuKiw9VTvo8AMw', # Trouble- Free\n",
    "    # 'UCmNXJXWONLNF6bdftGY0Otw', # Codanics\n",
    "    # # -------------------- 81-100 --------------------\n",
    "    \n",
    "    # # ==================== danieltran.ytapi ====================\n",
    "    # 'UCakdSIPsJqiOLqylgoYmwQg', # itversity\n",
    "    # 'UCsh8qhZ4Wm2IJDRsNr_5Z0A', # Smitha Kolan - Machine Learning Engineer\n",
    "    # 'UChIaUcs3tho6XhyU6K6KMrw', # Machine Learning TV\n",
    "    # 'UC7HYxRWmaNlJux-X7rNLZyw', # Tableau Tim\n",
    "    # 'UCL2ls5uXExB4p6_aZF2rUyg', # Anthony Smoak\n",
    "    # 'UCNJJIRGlnpS6yytn-9ADCOw', # Data Folkz\n",
    "    # 'UC4lrlpag0yO52XPhCmONXnw', # ViSIT\n",
    "    # 'UCRhhFunXogiEK3WiinHGTAQ', # MyStudy\n",
    "    # 'UCjrGJITO_pggWmjgPvUiHFA', # Arpan Gupta Data Scientist, IITian\n",
    "    # 'UCG6qpjVnBTTT8wLGBygANOQ', # MLOps.community\n",
    "    # 'UCBp3w4DCEC64FZr4k9ROxig', # Weights & Biases\n",
    "    # 'UCmKaoNn0OvxVAe7f_8sXYNQ', # Jovian\n",
    "    # 'UCKRgi-HJDEq0a3nhlG2nQvg', # Ricardo Calix\n",
    "    # 'UCa0RTSXWyZdh7IciV9r-3ow', # The Data Scientist Show - Daliana Liu\n",
    "    # 'UCu8WF59Scx9f3H1N_FgZUwQ', # Automata Learning Lab\n",
    "    # 'UCMGDKvc8-06jmxRrhYLr1_g', # Equitable Equations\n",
    "    # 'UCcfngi7_ASuo5jdWX0bNauQ', # How to Power BI \n",
    "    # 'UCQID78IY6EOojr5RUdD47MQ', # Data Driven NYC\n",
    "    # 'UCWPCd6tPtoLJYzQQ681pe5Q', # ggnot2\n",
    "    # 'UCeTSg29X4ZzoTvuynSVmrCA', # Quantitative Social Science Data Analysis\n",
    "    # # -------------------- 101-120 --------------------\n",
    "\n",
    "    # # ==================== tulindao.ytapi ====================\n",
    "    # 'UC8r94_jZaoXv9qsgFwAdPQQ', # Mike Crowson\n",
    "    # 'UCu9fxVjTz5AJO7FR1upY02w', # Rajistics - data science, AI, and machine learning\n",
    "    # 'UCsOfIwAXj1fT6LDqEDEAb4g', # Goodly\n",
    "    # 'UCbXgNpp0jedKWcQiULLbDTA', # Patrick Loeber \n",
    "    # 'UCAezwIIm1SfsqdmbQI-65pA', # Data Council\n",
    "    # 'UCrY1Ro4UXwMib9Qug3eJNWA', # Kahan Data Solutions\n",
    "    # 'UCChmJrVa8kDg05JfCmxpLRw', # Darshil Parmar \n",
    "    # 'UCBGcs9XTL5U34oaSn_AsHqw', # E-Learning Bridge \n",
    "    # 'UCsKYXFnst0YUwAkR4m5J_fw', # TechLake\n",
    "    # 'UCwBs8TLOogwyGd0GxHCp-Dw', # AIEngineering\n",
    "    # 'UCpNUYWW0kiqyh0j5Qy3aU7w', # Mısra Turp\n",
    "    # 'UCP7jMXSY2xbc3KCAE0MHQ-A', # Google DeepMind\n",
    "    # 'UCpABUkWm8xMt5XmGcFb3EFg', # Nicolai Nielsen\n",
    "    # 'UCAlwrsgeJavG1vw9qSFOUmA', # Analytics India Magazine\n",
    "    # 'UCvjgXvBlbQiydffZU7m1_aw', # The Coding Train\n",
    "    # 'UCSHZKyawb77ixDdsGog4iWA', # Lex Fridman\n",
    "    # 'UCHB9VepY6kYvZjj0Bgxnpbw', # Connor Shorten\n",
    "    # 'UCEqgmyWChwvt6MFGGlmUQCQ', # Allen Institute for AI\n",
    "    # 'UCupQLyNchb9-2Z5lmUOIijw', # Alfredo Canziani\n",
    "    # 'UCCGoM_sk2UGIiaTdtG3tHBw', # Sreyobhilashi IT\n",
    "    # # -------------------- 121-140 --------------------\n",
    "\n",
    "    # ==================== martincung.ytapi ====================\n",
    "    'UCbfYPyITQ-7l4upoX8nvctg', # Two Minute Papers \n",
    "    'UCdngmbVKX1Tgre699-XLlUA', # TechWorld with Nana\n",
    "    'UCNbfqCkmHEyf1CVKjuhEW_A', # DataEng Uncomplicated\n",
    "    'UCQIMjZigvDj6tWFoMTsN5_g', # Penguin Analytics\n",
    "    'UC9LfrPNcIyHspci0t2W4T_w', # Data36 - Online Data Science Courses\n",
    "    'UC_lePY0Lm0E2-_IkYUWpI5A', # Dataquest\n",
    "    'UCY66vV1WTk_2lHg24cuJrtg', # The Data Digest\n",
    "    'UC5zx8Owijmv-bbhAK6Z9apg', # Artificial Intelligence - All in One\n",
    "    'UC7kjWIK1H8tfmFlzZO-wHMw', # The TWIML AI Podcast with Sam Charrington\n",
    "    'UCrBzGHKmGDcwLFnQGHJ3XYg', # giant_neural_network\n",
    "    'UC12LqyqTQYbXatYS9AA7Nuw', # Machine Learning University\n",
    "    'UCv83tO5cePwHMt1952IVVHw', # James Briggs\n",
    "    'UCHz35rvIKf2CMqj7oiMv9WQ', # DecisionForest\n",
    "    'UCHxC-PYozvkqT0qgjkL2w3Q', # The NLP Lab\n",
    "    'UClKKWBe2SCAEyv7ZNGhIe4g', # Orange Data Mining\n",
    "    'UCqF6b0pT7OJCjCnCn4r3J4Q', # Leon Lok\n",
    "    'UCAWsBMQY4KSuOuGODki-l7A', # Emma Ding\n",
    "    'UCcaVCbOftKsfaY10HNHV52A', # Data Analytics.m\n",
    "    'UCHBWJGoZMkhJyElgvuN1U1w', # Art of Visualization\n",
    "    'UCtoNXlIegvxkvf5Ji8S57Ag', # Lore So What\n",
    "    # -------------------- 141-160 --------------------\n",
    "]\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(channel_ids) == len(set(channel_ids))\n",
    "len(channel_ids), len(set(channel_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Keep the max `Ids` in `channel_ids` per request at 50 before calling function `get_channel_stats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(channel_ids))\n",
    "    response = request.execute() \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelName = response['items'][i]['snippet']['title'],\n",
    "                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
    "                    views = response['items'][i]['statistics']['viewCount'],\n",
    "                    totalVideos = response['items'][i]['statistics']['videoCount'],\n",
    "                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get list of video IDs of all videos in the given playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    playlist_id: playlist ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    List of video IDs of all videos in the playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId = playlist_id,\n",
    "                maxResults = 50)\n",
    "    response = request.execute()\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "        \n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        if next_page_token is None:\n",
    "            more_pages = False\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                        part='contentDetails',\n",
    "                        playlistId = playlist_id,\n",
    "                        maxResults = 50,\n",
    "                        pageToken = next_page_token)\n",
    "            response = request.execute()\n",
    "    \n",
    "            for i in range(len(response['items'])):\n",
    "                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "    return video_ids\n",
    "\n",
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get video statistics of all videos with given IDs\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with statistics of videos, i.e.:\n",
    "        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n",
    "        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n",
    "        'duration', 'definition', 'caption'\n",
    "    \"\"\"\n",
    "        \n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute() \n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "                             'contentDetails': ['duration', 'definition', 'caption']\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "            \n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "def get_comments_in_videos(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with video IDs and associated top level comment in text.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    err_counter = 0 # To stop the loop when too many errors occur\n",
    "    for video_id in video_ids:\n",
    "        try:   \n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "        \n",
    "            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "\n",
    "            all_comments.append(comments_in_video_info)\n",
    "            \n",
    "        except: \n",
    "            # When error occurs - most likely because comments are disabled on a video\n",
    "            print('Could not get comments for video ' + video_id)\n",
    "            err_counter += 1\n",
    "            if err_counter >= 20:\n",
    "                break\n",
    "        \n",
    "    return pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data = get_channel_stats(youtube, channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"[LOG] Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "\n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "    # get comment data\n",
    "    comments_data = get_comments_in_videos(youtube, video_ids)\n",
    "\n",
    "    # append video data together and comment data toghether\n",
    "    # append row of video data to video_df\n",
    "    video_df = pd.concat([video_df, video_data])\n",
    "    comments_df = pd.concat([comments_df, comments_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.to_csv('data/video_raw_data_141_160.csv', index=False, na_rep=\"None\")\n",
    "comments_df.to_csv('data/comment_raw_data_141_160.csv', index=False, na_rep=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nối dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get each video componet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "video_component_1 = pd.read_csv(\"../data/raw/video_component/video_raw_data_1_20.csv\")\n",
    "video_component_2 = pd.read_csv(\"../data/raw/video_component/video_raw_data_21_40.csv\")\n",
    "video_component_3 = pd.read_csv(\"../data/raw/video_component/video_raw_data_41_60.csv\")\n",
    "video_component_4 = pd.read_csv(\"../data/raw/video_component/video_raw_data_61_80.csv\")\n",
    "video_component_5 = pd.read_csv(\"../data/raw/video_component/video_raw_data_81_100.csv\")\n",
    "video_component_6 = pd.read_csv(\"../data/raw/video_component/video_raw_data_101_120.csv\")\n",
    "video_component_7 = pd.read_csv(\"../data/raw/video_component/video_raw_data_121_140.csv\")\n",
    "video_component_8 = pd.read_csv(\"../data/raw/video_component/video_raw_data_141_160.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_video_df = pd.concat([video_component_1, video_component_2, video_component_3, \n",
    "                            video_component_4, video_component_5, video_component_6, \n",
    "                            video_component_7, video_component_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View some general description about our `final_video_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 60032 entries, 0 to 4120\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   video_id        60032 non-null  object \n",
      " 1   channelTitle    60032 non-null  object \n",
      " 2   title           60032 non-null  object \n",
      " 3   description     58349 non-null  object \n",
      " 4   tags            48652 non-null  object \n",
      " 5   publishedAt     60032 non-null  object \n",
      " 6   viewCount       60028 non-null  float64\n",
      " 7   likeCount       59789 non-null  float64\n",
      " 8   favouriteCount  0 non-null      float64\n",
      " 9   commentCount    59264 non-null  float64\n",
      " 10  duration        60032 non-null  object \n",
      " 11  definition      60032 non-null  object \n",
      " 12  caption         60032 non-null  bool   \n",
      "dtypes: bool(1), float64(4), object(8)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "final_video_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60032,)\n"
     ]
    }
   ],
   "source": [
    "assert final_video_df[\"video_id\"].value_counts(dropna=False).shape[0] == final_video_df.shape[0]\n",
    "print(final_video_df[\"video_id\"].value_counts(dropna=False).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `final raw video dateframe` for future processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_video_df.to_csv(\"../data/raw/video_data_raw.csv\", index=False, na_rep=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60032 entries, 0 to 60031\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   video_id        60032 non-null  object \n",
      " 1   channelTitle    60032 non-null  object \n",
      " 2   title           60032 non-null  object \n",
      " 3   description     58349 non-null  object \n",
      " 4   tags            48652 non-null  object \n",
      " 5   publishedAt     60032 non-null  object \n",
      " 6   viewCount       60028 non-null  float64\n",
      " 7   likeCount       59789 non-null  float64\n",
      " 8   favouriteCount  0 non-null      float64\n",
      " 9   commentCount    59264 non-null  float64\n",
      " 10  duration        60032 non-null  object \n",
      " 11  definition      60032 non-null  object \n",
      " 12  caption         60032 non-null  bool   \n",
      "dtypes: bool(1), float64(4), object(8)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "check_final_video_df = pd.read_csv(\"../data/raw/video_data_raw.csv\")\n",
    "check_final_video_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get each comment componet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_component_1 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_1_20.csv\")\n",
    "comment_component_2 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_21_40.csv\")\n",
    "comment_component_3 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_41_60.csv\")\n",
    "comment_component_4 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_61_80.csv\")\n",
    "comment_component_5 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_81_100.csv\")\n",
    "comment_component_6 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_101_120.csv\")\n",
    "comment_component_7 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_121_140.csv\")\n",
    "comment_component_8 = pd.read_csv(\"../data/raw/comment_component/comment_raw_data_141_160.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comment_df \\\n",
    "    = pd.concat([comment_component_1, comment_component_2, comment_component_3, \n",
    "                 comment_component_4, comment_component_5, comment_component_6, \n",
    "                 comment_component_7, comment_component_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View some general description about our `final_comment_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 55309 entries, 0 to 3978\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   video_id  55309 non-null  object\n",
      " 1   comments  55309 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "final_comment_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55309,)\n"
     ]
    }
   ],
   "source": [
    "assert final_comment_df[\"video_id\"].value_counts(dropna=False).shape[0] == final_comment_df.shape[0]\n",
    "print(final_comment_df[\"video_id\"].value_counts(dropna=False).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `final raw comment dateframe` for future processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comment_df.to_csv(\"../data/raw/comment_data_raw.csv\", index=False, na_rep=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55309 entries, 0 to 55308\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   video_id  55309 non-null  object\n",
      " 1   comments  55309 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 864.3+ KB\n"
     ]
    }
   ],
   "source": [
    "check_final_comment_df = pd.read_csv(\"../data/raw/comment_data_raw.csv\")\n",
    "check_final_comment_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_video_ids = check_final_video_df[\"video_id\"].unique()\n",
    "video_video_ids   = check_final_comment_df[\"video_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if all video_id in comment_df are in video_df\n",
    "# assert np.isin(comment_video_ids, video_video_ids).all() == True # of course\n",
    "\n",
    "# # Check if all video_id in video_df are in comment_df\n",
    "# np.isin(video_video_ids, comment_video_ids).all() # expected equal False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
